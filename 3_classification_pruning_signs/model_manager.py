import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
import os
import numpy as np
from typing import Tuple, Dict, List
import timm


class ModelManager:
    """
    Handles creation, training, and evaluation of neural network models.
    Supports lightweight CNNs, hybrid architectures, and Vision Transformers
    suitable for edge devices and federated learning clients.

    Model categories:
      CNN (pure):   ResNet18, EfficientNet_B0, ConvNeXt_Atto
      Hybrid:       MobileViT_Small, EdgeNeXt_Small, EfficientFormer_L1
      ViT (pure):   DeiT_Tiny, TinyViT_5M, EfficientViT_M2
    """

    # Mapping from internal model names to timm identifiers
    TIMM_MODEL_NAMES = {
        # --- CNN pure ---
        'EfficientNet_B0':    'efficientnet_b0',
        'ConvNeXt_Atto':      'convnext_atto',
        # --- Hybrid CNN + Transformer ---
        'MobileViT_Small':    'mobilevit_s',
        'EdgeNeXt_Small':     'edgenext_small',
        'EfficientFormer_L1': 'efficientformer_l1',
        # --- ViT pure ---
        'DeiT_Tiny':          'deit_tiny_patch16_224',
        'TinyViT_5M':         'tiny_vit_5m_224',
        'FastViT_T12':        'fastvit_t12',
    }

    def __init__(self, config: Dict, dataset_path: str):
        self.config = config
        self.model_name = config.get('model_name', 'ResNet18')
        self.dataset_path = dataset_path
        self.num_classes = config.get('num_classes', 2)
        self.device = self._get_device(config.get('device', 'cuda'))
        self.model = self._initialize_model().to(self.device)
        self.criterion = nn.CrossEntropyLoss()
        self.transform_pipeline = self._get_transforms()
        self.calibration_term = torch.zeros(self.num_classes, device=self.device)

    def _get_device(self, device_str: str) -> torch.device:
        if device_str == 'cuda' and torch.cuda.is_available():
            return torch.device("cuda:0")
        return torch.device("cpu")

    def _get_transforms(self) -> transforms.Compose:
        image_size = self.config.get('image_size', 224)
        return transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

    def _initialize_model(self) -> nn.Module:
        print(f"Initializing model: {self.model_name}")
        if self.model_name == 'ResNet18':
            return self._initialize_resnet()
        elif self.model_name in self.TIMM_MODEL_NAMES:
            return self._initialize_timm_model()
        else:
            raise ValueError(f"Model '{self.model_name}' is not supported. "
                             f"Available models: ResNet18, {', '.join(self.TIMM_MODEL_NAMES.keys())}")

    def _initialize_resnet(self) -> nn.Module:
        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        num_features = model.fc.in_features
        return self._apply_custom_head(model, num_features, 'fc')

    def _apply_custom_head(self, model: nn.Module, num_features: int, classifier_name: str) -> nn.Module:
        """
        Replaces the classifier head of a torchvision model.
        If num_custom_layers is 0, all backbone parameters remain trainable;
        otherwise the backbone is frozen and only the new head is trained.
        """
        num_custom_layers = self.config.get('num_custom_layers', 2)
        train_full_model = (num_custom_layers == 0)

        if train_full_model:
            print(f"num_custom_layers is 0. All {self.model_name} parameters will be trainable.")
        else:
            print(f"Freezing pre-trained {self.model_name} layers. Only custom head will be trained.")

        for param in model.parameters():
            param.requires_grad = train_full_model

        if num_custom_layers > 0:
            layers = self._create_custom_classifier(num_features, self.num_classes, num_custom_layers)
            setattr(model, classifier_name, nn.Sequential(*layers))
        else:
            setattr(model, classifier_name, nn.Linear(num_features, self.num_classes))

        # Ensure the new head is always trainable regardless of backbone freezing
        for param in getattr(model, classifier_name).parameters():
            param.requires_grad = True

        return model

    def _initialize_timm_model(self) -> nn.Module:
        """
        Loads a pretrained timm model. If num_custom_layers is 0, the full model is
        fine-tuned end-to-end. Otherwise, the backbone is frozen and a custom
        classification head is attached after inferring the feature dimension.
        """
        actual_name = self.TIMM_MODEL_NAMES[self.model_name]
        num_custom_layers = self.config.get('num_custom_layers', 2)
        train_full_model = (num_custom_layers == 0)

        if train_full_model:
            print(f"Loading {self.model_name} (all trainable) with {self.num_classes} classes.")
            return timm.create_model(actual_name, pretrained=True, num_classes=self.num_classes)

        print(f"Freezing pre-trained {self.model_name} layers. Adding custom head ({num_custom_layers} layers).")

        # Load backbone without classification head to infer output feature size
        backbone = timm.create_model(actual_name, pretrained=True, num_classes=0)
        image_size = self.config.get('image_size', 224)
        dummy_input = torch.randn(1, 3, image_size, image_size)
        with torch.no_grad():
            in_features = backbone(dummy_input).shape[1]

        for param in backbone.parameters():
            param.requires_grad = False

        custom_head = nn.Sequential(*self._create_custom_classifier(in_features, self.num_classes, num_custom_layers))
        return nn.Sequential(backbone, custom_head)

    def _create_custom_classifier(self, in_features: int, out_features: int, num_layers: int) -> List[nn.Module]:
        """
        Builds a fully-connected classification head with optional hidden layers.
        Each hidden layer halves the feature dimension and applies ReLU + Dropout.
        """
        layers = []
        if num_layers < 2:
            layers.append(nn.Linear(in_features, out_features))
        else:
            hidden_size = 256
            layers.extend([nn.Linear(in_features, hidden_size), nn.ReLU(), nn.Dropout(0.4)])
            in_size = hidden_size
            for _ in range(num_layers - 2):
                out_size = in_size // 2
                layers.extend([nn.Linear(in_size, out_size), nn.ReLU(), nn.Dropout(0.4)])
                in_size = out_size
            layers.append(nn.Linear(in_size, out_features))
        return layers

    def _get_trainable_parameters(self) -> List[torch.Tensor]:
        return [p for p in self.model.parameters() if p.requires_grad]

    def get_weights(self) -> List[np.ndarray]:
        """Returns the current trainable weights as a list of numpy arrays."""
        return [param.data.cpu().numpy() for param in self._get_trainable_parameters()]

    def set_weights(self, weights: List[np.ndarray]) -> None:
        """Loads a list of numpy weight arrays into the model's trainable parameters."""
        trainable_params = self._get_trainable_parameters()
        for param, weight_array in zip(trainable_params, weights):
            param.data.copy_(torch.from_numpy(weight_array))

    def get_samples_per_class(self) -> np.ndarray:
        """Counts and returns the number of training samples per class."""
        samples = torch.zeros(self.num_classes, device=self.device)
        try:
            train_loader = self._get_dataloader('train', batch_size=32)
            for _, labels in train_loader:
                for label in labels:
                    samples[label.item()] += 1
        except FileNotFoundError:
            print("Warning: Training data not found during sample count.")
        return samples.cpu().numpy()

    def set_calibration_term(self, calibration_val: np.ndarray) -> None:
        """Sets the FedLC calibration term from a numpy array."""
        self.calibration_term = torch.from_numpy(calibration_val).float().to(self.device)

    def _get_dataloader(self, split: str, batch_size: int) -> DataLoader:
        data_path = os.path.join(self.dataset_path, split)
        if not os.path.isdir(data_path):
            raise FileNotFoundError(f"Dataset directory not found for split '{split}': {data_path}")
        dataset = ImageFolder(root=data_path, transform=self.transform_pipeline)
        return DataLoader(dataset, batch_size=batch_size, shuffle=(split == 'train'))

    def train(self, epochs: int, lr: float, batch_size: int,
              algorithm: str = "FedAvg", global_weights: List[np.ndarray] = None,
              mu: float = 0.0) -> Tuple[float, Dict, float, int]:
        """
        Trains the model for a given number of epochs and returns metrics on the training set.
        Supports FedAvg, FedProx (with proximal regularization), and FedLC (with calibration).
        """
        trainable_params = self._get_trainable_parameters()
        optimizer = torch.optim.Adam(trainable_params, lr=lr)
        train_loader = self._get_dataloader('train', batch_size)

        # Pre-convert global weights to tensors once if FedProx regularization is needed
        global_params_tensor = None
        if algorithm == 'FedProx' and global_weights is not None:
            global_params_tensor = [torch.from_numpy(w).to(self.device) for w in global_weights]

        final_train_loss = 0.0
        for epoch in range(epochs):
            final_train_loss = self._run_training_epoch(train_loader, optimizer, algorithm, global_params_tensor, mu)

        _, metric_score, _, dataset_size = self.validate(batch_size, split='train')
        return 0.0, metric_score, final_train_loss, dataset_size

    def _run_training_epoch(self, loader: DataLoader, optimizer: torch.optim.Optimizer,
                            algorithm: str, global_params: List[torch.Tensor], mu: float) -> float:
        """Runs one full training epoch and returns the average loss over the dataset."""
        self.model.train()
        running_loss = 0.0
        for inputs, labels in loader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            optimizer.zero_grad()
            outputs = self.model(inputs)

            if algorithm == 'FedLC':
                # Subtract calibration term to adjust logits based on class frequency
                loss = self.criterion(outputs - self.calibration_term.unsqueeze(0), labels)
            else:
                loss = self.criterion(outputs, labels)

            if algorithm == 'FedProx' and mu > 0 and global_params:
                # Add proximal term to penalize deviation from the global model
                prox_term = sum((lw - gw).norm(2) for lw, gw in zip(self._get_trainable_parameters(), global_params))
                loss += (mu / 2) * prox_term

            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)

        return running_loss / len(loader.dataset)

    def validate(self, batch_size: int, split: str = 'valid') -> Tuple[float, Dict, float, int]:
        """
        Evaluates the model on the specified data split.
        Returns loss, a metrics dict (F1, accuracy, precision, recall), a placeholder, and dataset size.
        """
        try:
            data_loader = self._get_dataloader(split, batch_size)
        except FileNotFoundError:
            return 0.0, {'f1_score': 0, 'accuracy': 0, 'precision': 0, 'recall': 0}, 0.0, 0

        self.model.eval()
        all_preds, all_labels = [], []
        running_loss = 0.0

        with torch.no_grad():
            for images, labels in data_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                running_loss += loss.item() * images.size(0)
                _, preds = torch.max(outputs, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        val_loss = running_loss / len(data_loader.dataset)
        metrics = {
            'f1_score': f1_score(all_labels, all_preds, average='weighted', zero_division=0),
            'accuracy': accuracy_score(all_labels, all_preds),
            'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),
            'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0)
        }
        return val_loss, metrics, 0.0, len(data_loader.dataset)